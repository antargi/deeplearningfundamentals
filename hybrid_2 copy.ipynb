{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import dask\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 47MB\n",
      "Dimensions:              (time: 124, latitude: 157, longitude: 121)\n",
      "Coordinates:\n",
      "  * latitude             (latitude) float32 628B -17.0 -17.25 ... -55.75 -56.0\n",
      "    level                int64 8B ...\n",
      "  * longitude            (longitude) float32 484B 280.0 280.2 ... 309.8 310.0\n",
      "  * time                 (time) datetime64[ns] 992B 2010-01-01 ... 2010-01-31...\n",
      "Data variables:\n",
      "    geopotential         (time, latitude, longitude) float32 9MB dask.array<chunksize=(10, 50, 50), meta=np.ndarray>\n",
      "    specific_humidity    (time, latitude, longitude) float32 9MB dask.array<chunksize=(10, 50, 50), meta=np.ndarray>\n",
      "    temperature          (time, latitude, longitude) float32 9MB dask.array<chunksize=(10, 50, 50), meta=np.ndarray>\n",
      "    u_component_of_wind  (time, latitude, longitude) float32 9MB dask.array<chunksize=(10, 50, 50), meta=np.ndarray>\n",
      "    v_component_of_wind  (time, latitude, longitude) float32 9MB dask.array<chunksize=(10, 50, 50), meta=np.ndarray>\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_zarr(\"dataset/chile_2010_january.zarr\")\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(ds, variables, train_time_scale, val_time_scale, test_time_scale):\n",
    "    train_vars = [ds[var].sel(time=train_time_scale).values for var in variables]  # Entrenamiento\n",
    "    val_vars = [ds[var].sel(time=val_time_scale).values for var in variables]     # Validación\n",
    "    test_vars = [ds[var].sel(time=test_time_scale).values for var in variables]   # Prueba\n",
    "\n",
    "    train_data = np.array(train_vars)\n",
    "    val_data = np.array(val_vars)\n",
    "    test_data = np.array(test_vars)\n",
    "\n",
    "    train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "    val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "    test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "    lat, lon = train_data.shape[2], train_data.shape[3]\n",
    "\n",
    "    means = train_data.mean(dim=(1, 2, 3), keepdim=True)\n",
    "    stds = train_data.std(dim=(1, 2, 3), keepdim=True)\n",
    "    \n",
    "    train_data = (train_data - means) / stds\n",
    "    val_data = (val_data - means) / stds\n",
    "    test_data = (test_data - means) / stds\n",
    "\n",
    "    return train_data, val_data, test_data, lat, lon, means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_time_scale = slice(\"2010-01-01\", \"2010-01-20\")\n",
    "val_time_scale = slice(\"2010-01-21\", \"2010-01-25\")\n",
    "test_time_scale = slice(\"2010-01-26\", \"2010-01-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['geopotential', 'specific_humidity', 'temperature', 'u_component_of_wind', 'v_component_of_wind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: torch.Size([80, 5, 157, 121])\n",
      "Validation Data Shape: torch.Size([20, 5, 157, 121])\n",
      "Test Data Shape: torch.Size([24, 5, 157, 121])\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, lat, lon, means, stds = preprocess_dataset(\n",
    "    ds, \n",
    "    variables, \n",
    "    train_time_scale, \n",
    "    val_time_scale, \n",
    "    test_time_scale\n",
    ")\n",
    "\n",
    "train_data = train_data.permute(1, 0, 2, 3)\n",
    "val_data = val_data.permute(1, 0, 2, 3)\n",
    "test_data = test_data.permute(1, 0, 2, 3)\n",
    "\n",
    "print(\"Train Data Shape:\", train_data.shape)\n",
    "print(\"Validation Data Shape:\", val_data.shape)\n",
    "print(\"Test Data Shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch Shape: torch.Size([8, 5, 157, 121])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(train_data)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "test_dataset = TensorDataset(test_data)\n",
    "\n",
    "batch_size = 8  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(\"Train Batch Shape:\", batch[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleClimateModel(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(SimpleClimateModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, output_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=5):\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_data = batch[0].to(device)\n",
    "            output = model(input_data)\n",
    "            loss = loss_fn(output, input_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_data = batch[0].to(device)\n",
    "                output = model(input_data)\n",
    "                loss = loss_fn(output, input_data)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Imprimir pérdidas\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Training Loss: 0.8327, Validation Loss: 0.5052\n",
      "Epoch 2/5, Training Loss: 0.3452, Validation Loss: 0.2573\n",
      "Epoch 3/5, Training Loss: 0.2020, Validation Loss: 0.1568\n",
      "Epoch 4/5, Training Loss: 0.1196, Validation Loss: 0.0882\n",
      "Epoch 5/5, Training Loss: 0.0801, Validation Loss: 0.0674\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "model = SimpleClimateModel(input_channels, output_channels)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss() \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_model(model, train_loader, val_loader, optimizer, loss_fn, device, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_data = batch[0].to(device)\n",
    "            output = model(input_data)\n",
    "            loss = loss_fn(output, input_data)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0814\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate_model(model, test_loader, loss_fn, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.2078\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_mae = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_data = batch[0].to(device)\n",
    "            output = model(input_data)\n",
    "            mae = torch.abs(output - input_data).mean().item()\n",
    "            total_mae += mae\n",
    "    return total_mae / len(data_loader)\n",
    "\n",
    "test_mae = mean_absolute_error(model, test_loader, device)\n",
    "print(f\"Mean Absolute Error (MAE): {test_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spatial_embeddings(latitudes, longitudes):\n",
    "    \"\"\"\n",
    "    Genera embeddings trigonométricos para latitud y longitud.\n",
    "    \"\"\"\n",
    "    lat_rad = torch.tensor(np.radians(latitudes), dtype=torch.float32)  # Convertir a radianes\n",
    "    lon_rad = torch.tensor(np.radians(longitudes), dtype=torch.float32)\n",
    "\n",
    "    # Calcular embeddings trigonométricos   \n",
    "    sin_lat = torch.sin(lat_rad).unsqueeze(1)  # Shape: (lat, 1)\n",
    "    cos_lat = torch.cos(lat_rad).unsqueeze(1)\n",
    "    sin_lon = torch.sin(lon_rad).unsqueeze(0)  # Shape: (1, lon)\n",
    "    cos_lon = torch.cos(lon_rad).unsqueeze(0)\n",
    "\n",
    "    # Combinar embeddings en una cuadrícula\n",
    "    spatial_embeddings = torch.cat([\n",
    "        sin_lat * cos_lon, sin_lat * sin_lon,\n",
    "        cos_lat * cos_lon, cos_lat * sin_lon\n",
    "    ], dim=0)  # Shape: (4, lat, lon)\n",
    "\n",
    "    return spatial_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temporal_embeddings(time_steps):\n",
    "    \"\"\"\n",
    "    Genera embeddings temporales usando funciones trigonométricas.\n",
    "    \"\"\"\n",
    "    day_of_year = (time_steps.dayofyear / 365.0) * 2 * np.pi\n",
    "    hour_of_day = (time_steps.hour / 24.0) * 2 * np.pi\n",
    "\n",
    "    # Embeddings seno y coseno\n",
    "    sin_day = torch.sin(torch.tensor(day_of_year, dtype=torch.float32)).unsqueeze(1)\n",
    "    cos_day = torch.cos(torch.tensor(day_of_year, dtype=torch.float32)).unsqueeze(1)\n",
    "    sin_hour = torch.sin(torch.tensor(hour_of_day, dtype=torch.float32)).unsqueeze(1)\n",
    "    cos_hour = torch.cos(torch.tensor(hour_of_day, dtype=torch.float32)).unsqueeze(1)\n",
    "\n",
    "    # Combinar embeddings\n",
    "    temporal_embeddings = torch.cat([sin_day, cos_day, sin_hour, cos_hour], dim=1)  # Shape: (time, 4)\n",
    "    return temporal_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
