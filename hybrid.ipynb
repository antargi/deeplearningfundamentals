{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchdiffeq import odeint\n",
    "import xarray as xr\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 41TB\n",
      "Dimensions:                                           (time: 92044,\n",
      "                                                       latitude: 721,\n",
      "                                                       longitude: 1440,\n",
      "                                                       level: 13)\n",
      "Coordinates:\n",
      "  * latitude                                          (latitude) float32 3kB ...\n",
      "  * level                                             (level) int64 104B 50 ....\n",
      "  * longitude                                         (longitude) float32 6kB ...\n",
      "  * time                                              (time) datetime64[ns] 736kB ...\n",
      "Data variables: (12/38)\n",
      "    10m_u_component_of_wind                           (time, latitude, longitude) float32 382GB ...\n",
      "    10m_v_component_of_wind                           (time, latitude, longitude) float32 382GB ...\n",
      "    10m_wind_speed                                    (time, latitude, longitude) float32 382GB ...\n",
      "    2m_temperature                                    (time, latitude, longitude) float32 382GB ...\n",
      "    angle_of_sub_gridscale_orography                  (latitude, longitude) float32 4MB ...\n",
      "    anisotropy_of_sub_gridscale_orography             (latitude, longitude) float32 4MB ...\n",
      "    ...                                                ...\n",
      "    type_of_high_vegetation                           (latitude, longitude) float32 4MB ...\n",
      "    type_of_low_vegetation                            (latitude, longitude) float32 4MB ...\n",
      "    u_component_of_wind                               (time, level, latitude, longitude) float32 5TB ...\n",
      "    v_component_of_wind                               (time, level, latitude, longitude) float32 5TB ...\n",
      "    vertical_velocity                                 (time, level, latitude, longitude) float32 5TB ...\n",
      "    wind_speed                                        (time, level, latitude, longitude) float32 5TB ...\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'gs://weatherbench2/datasets/era5/1959-2022-6h-1440x721.zarr'\n",
    "ds = xr.open_zarr(dataset_path)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variables disponibles:\n",
      "['10m_u_component_of_wind', '10m_v_component_of_wind', '10m_wind_speed', '2m_temperature', 'angle_of_sub_gridscale_orography', 'anisotropy_of_sub_gridscale_orography', 'geopotential', 'geopotential_at_surface', 'high_vegetation_cover', 'lake_cover', 'lake_depth', 'land_sea_mask', 'low_vegetation_cover', 'mean_sea_level_pressure', 'sea_ice_cover', 'sea_surface_temperature', 'slope_of_sub_gridscale_orography', 'soil_type', 'specific_humidity', 'standard_deviation_of_filtered_subgrid_orography', 'standard_deviation_of_orography', 'surface_pressure', 'temperature', 'toa_incident_solar_radiation', 'toa_incident_solar_radiation_12hr', 'toa_incident_solar_radiation_24hr', 'toa_incident_solar_radiation_6hr', 'total_cloud_cover', 'total_column_water_vapour', 'total_precipitation_12hr', 'total_precipitation_24hr', 'total_precipitation_6hr', 'type_of_high_vegetation', 'type_of_low_vegetation', 'u_component_of_wind', 'v_component_of_wind', 'vertical_velocity', 'wind_speed']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVariables disponibles:\")\n",
    "print(list(ds.data_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 18MB\n",
      "Dimensions:                  (time: 8, latitude: 157, longitude: 121, level: 13)\n",
      "Coordinates:\n",
      "  * latitude                 (latitude) float32 628B -17.0 -17.25 ... -56.0\n",
      "  * level                    (level) int64 104B 50 100 150 200 ... 850 925 1000\n",
      "  * longitude                (longitude) float32 484B 280.0 280.2 ... 310.0\n",
      "  * time                     (time) datetime64[ns] 64B 2010-01-01 ... 2010-01...\n",
      "Data variables:\n",
      "    10m_u_component_of_wind  (time, latitude, longitude) float32 608kB ...\n",
      "    10m_v_component_of_wind  (time, latitude, longitude) float32 608kB ...\n",
      "    2m_temperature           (time, latitude, longitude) float32 608kB ...\n",
      "    geopotential             (time, level, latitude, longitude) float32 8MB ...\n",
      "    specific_humidity        (time, level, latitude, longitude) float32 8MB ...\n"
     ]
    }
   ],
   "source": [
    "variables = [\n",
    "    '10m_u_component_of_wind',\n",
    "    '10m_v_component_of_wind',\n",
    "    '2m_temperature',\n",
    "    'geopotential',\n",
    "    'specific_humidity'\n",
    "]\n",
    "subset = ds[variables].sel(\n",
    "    time=slice('2010-01-01', '2010-01-02'),\n",
    "    latitude=slice(-17.0, -56.0),          \n",
    "    longitude=slice(280.0, 310.0) \n",
    ")\n",
    "\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(subset, variables):\n",
    "    normalized_vars = {}\n",
    "    for var in variables:\n",
    "        data = subset[var].values  # Extraer datos\n",
    "        mean = data.mean()\n",
    "        std = data.std()\n",
    "        normalized_data = (data - mean) / std\n",
    "        normalized_vars[var] = normalized_data\n",
    "\n",
    "    tensors = [torch.tensor(normalized_vars[var], dtype=torch.float32) for var in variables]\n",
    "    tensor_data = torch.stack(tensors, dim=1) \n",
    "    return tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: 10m_u_component_of_wind, Shape: (8, 157, 121)\n",
      "Variable: 10m_v_component_of_wind, Shape: (8, 157, 121)\n",
      "Variable: 2m_temperature, Shape: (8, 157, 121)\n",
      "Variable: geopotential, Shape: (8, 13, 157, 121)\n",
      "Variable: specific_humidity, Shape: (8, 13, 157, 121)\n"
     ]
    }
   ],
   "source": [
    "for var in variables:\n",
    "    print(f\"Variable: {var}, Shape: {subset[var].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['geopotential'] = subset['geopotential'].sel(level=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset['specific_humidity'] = subset['specific_humidity'].sel(level=subset['specific_humidity']['level'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = preprocess_dataset(subset, variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: torch.Size([5, 5, 157, 121]), Validación: torch.Size([1, 5, 157, 121]), Prueba: torch.Size([2, 5, 157, 121])\n"
     ]
    }
   ],
   "source": [
    "# División\n",
    "time_steps = data_tensor.shape[0]\n",
    "train_split = int(0.7 * time_steps)\n",
    "val_split = int(0.15 * time_steps)\n",
    "\n",
    "train_data = data_tensor[:train_split]\n",
    "val_data = data_tensor[train_split:train_split + val_split]\n",
    "test_data = data_tensor[train_split + val_split:]\n",
    "\n",
    "print(f\"Entrenamiento: {train_data.shape}, Validación: {val_data.shape}, Prueba: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: 5\n",
      "Tamaño del conjunto de validación: 1\n",
      "Tamaño del conjunto de prueba: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño del conjunto de entrenamiento: {train_data.shape[0]}\")\n",
    "print(f\"Tamaño del conjunto de validación: {val_data.shape[0]}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {test_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventanas ajustadas - Entrenamiento: 5, Validación: 1, Prueba: 2\n"
     ]
    }
   ],
   "source": [
    "window_size_train = min(5, train_data.shape[0])\n",
    "window_size_val = min(5, val_data.shape[0])\n",
    "window_size_test = min(5, test_data.shape[0])\n",
    "\n",
    "print(f\"Ventanas ajustadas - Entrenamiento: {window_size_train}, Validación: {window_size_val}, Prueba: {window_size_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_windows(data, window_size, stride):\n",
    "  \"\"\"\n",
    "  Crea ventanas temporales de tamaño window_size con un stride específico.\n",
    "  \"\"\"\n",
    "  if data.shape[0] < window_size:\n",
    "      print(f\"No se pueden crear ventanas: tamaño insuficiente de datos ({data.shape[0]} < {window_size}).\")\n",
    "      return torch.empty(0)  # Retorna un tensor vacío si no hay suficientes datos\n",
    "\n",
    "  windows = []\n",
    "  for i in range(0, data.shape[0] - window_size + 1, stride):\n",
    "      windows.append(data[i:i + window_size])\n",
    "  return torch.stack(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ventanas de entrenamiento: torch.Size([1, 5, 5, 157, 121])\n",
      "Ventanas de validación: torch.Size([1, 1, 5, 157, 121])\n",
      "Ventanas de prueba: torch.Size([1, 2, 5, 157, 121])\n"
     ]
    }
   ],
   "source": [
    "train_windows = create_temporal_windows(train_data, window_size=5, stride=1)\n",
    "val_windows = create_temporal_windows(val_data, window_size=1, stride=1)\n",
    "test_windows = create_temporal_windows(test_data, window_size=2, stride=1)\n",
    "print(f\"Ventanas de entrenamiento: {train_windows.shape}\")\n",
    "print(f\"Ventanas de validación: {val_windows.shape}\")\n",
    "print(f\"Ventanas de prueba: {test_windows.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas ventanas de entrenamiento: torch.Size([3, 5, 5, 157, 121])\n",
      "Nuevas ventanas de validación: torch.Size([3, 1, 5, 157, 121])\n",
      "Nuevas ventanas de prueba: torch.Size([3, 2, 5, 157, 121])\n"
     ]
    }
   ],
   "source": [
    "def duplicate_windows(windows, factor=3):\n",
    "    \"\"\"\n",
    "    Duplica las ventanas existentes para crear un conjunto más grande.\n",
    "    \"\"\"\n",
    "    return torch.cat([windows for _ in range(factor)], dim=0)\n",
    "\n",
    "if train_windows.shape[0] > 0:\n",
    "    train_windows = duplicate_windows(train_windows, factor=3)\n",
    "\n",
    "if val_windows.shape[0] > 0:\n",
    "    val_windows = duplicate_windows(val_windows, factor=3)\n",
    "\n",
    "if test_windows.shape[0] > 0:\n",
    "    test_windows = duplicate_windows(test_windows, factor=3)\n",
    "\n",
    "print(f\"Nuevas ventanas de entrenamiento: {train_windows.shape}\")\n",
    "print(f\"Nuevas ventanas de validación: {val_windows.shape}\")\n",
    "print(f\"Nuevas ventanas de prueba: {test_windows.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lotes de entrenamiento: 2\n",
      "Lotes de validación: 2\n",
      "Lotes de prueba: 2\n"
     ]
    }
   ],
   "source": [
    "train_loader = None\n",
    "val_loader = None\n",
    "test_loader = None\n",
    "\n",
    "if train_windows.shape[0] > 1:\n",
    "    train_dataset = TensorDataset(train_windows[:-1], train_windows[1:])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "else:\n",
    "    print(\"Conjunto de entrenamiento insuficiente para crear DataLoader.\")\n",
    "\n",
    "if val_windows.shape[0] > 1:\n",
    "    val_dataset = TensorDataset(val_windows[:-1], val_windows[1:])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "else:\n",
    "    print(\"Conjunto de validación insuficiente para crear DataLoader.\")\n",
    "\n",
    "if test_windows.shape[0] > 1:\n",
    "    test_dataset = TensorDataset(test_windows[:-1], test_windows[1:])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "else:\n",
    "    print(\"Conjunto de prueba insuficiente para crear DataLoader.\")\n",
    "\n",
    "# Confirmar los DataLoaders creados\n",
    "if train_loader:\n",
    "    print(f\"Lotes de entrenamiento: {len(train_loader)}\")\n",
    "if val_loader:\n",
    "    print(f\"Lotes de validación: {len(val_loader)}\")\n",
    "if test_loader:\n",
    "    print(f\"Lotes de prueba: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloque Residual Simplificado\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Proyección para igualar dimensiones si in_channels != out_channels\n",
    "        self.projection = nn.Conv2d(in_channels, out_channels, kernel_size=1) if in_channels != out_channels else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.projection:\n",
    "            residual = self.projection(x)  # Proyectar para igualar dimensiones\n",
    "\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return self.relu(x + residual)\n",
    "\n",
    "\n",
    "# Modelo CNN Simplificado\n",
    "class ClimateResNet2D(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.resblock1 = ResidualBlock(in_channels, hidden_channels)\n",
    "        self.resblock2 = ResidualBlock(hidden_channels, hidden_channels)\n",
    "        self.output_layer = nn.Conv2d(hidden_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Restricciones PDE Simplificadas\n",
    "def pde_loss(pred, vx, vy):\n",
    "    \"\"\"\n",
    "    Calcula la pérdida basada en la ecuación de advección.\n",
    "    \"\"\"\n",
    "    pred.requires_grad_(True)\n",
    "    u_t = torch.gradient(pred, dim=0)[0]  # Gradiente respecto al tiempo\n",
    "    u_x = torch.gradient(pred, dim=2)[0]  # Gradiente respecto a x\n",
    "    u_y = torch.gradient(pred, dim=3)[0]  # Gradiente respecto a y\n",
    "\n",
    "    advection = u_t + vx * u_x + vy * u_y\n",
    "    return torch.mean(advection ** 2)\n",
    "\n",
    "class ODEBlock(nn.Module):\n",
    "    def __init__(self, func, features):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.features = features  # Número de características planas (channels * height * width)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Aplanar dimensiones espaciales\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.view(batch_size, -1)  # Aplanar a (batch_size, features)\n",
    "\n",
    "        # Pasar por el solver ODE\n",
    "        x = odeint(self.func, x, t, method='rk4')\n",
    "\n",
    "        # Restaurar dimensiones espaciales\n",
    "        x = x.view(batch_size, channels, height, width)\n",
    "        return x\n",
    "\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Modelo Final Simplificado\n",
    "class ClimateModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, height, width):\n",
    "        super().__init__()\n",
    "        self.cnn = ClimateResNet2D(in_channels, hidden_channels)\n",
    "        \n",
    "        # Calcular el número de características planas para el ODEBlock\n",
    "        features = hidden_channels * height * width\n",
    "        \n",
    "        # Inicializar ODEBlock con el número correcto de características\n",
    "        self.ode_block = ODEBlock(ODEFunc(features), features)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.cnn(x)  # CNN para efectos locales\n",
    "        x = self.ode_block(x, t)  # ODE para continuidad temporal\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, optimizer, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Aplanar las dimensiones (batch_size, window_size * channels, height, width)\n",
    "            batch_size, window_size, channels, height, width = inputs.shape\n",
    "            inputs = inputs.view(batch_size, window_size * channels, height, width)\n",
    "            targets = targets.view(batch_size, window_size * channels, height, width)\n",
    "\n",
    "            t = torch.linspace(0, 1, steps=inputs.size(1)).to(device)  # Normalizar tiempos\n",
    "\n",
    "            # Forward\n",
    "            outputs = model(inputs, t)\n",
    "\n",
    "            # Pérdida\n",
    "            loss = nn.functional.mse_loss(outputs, targets)\n",
    "\n",
    "            # Backpropagación\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "features = 64 * 157 * 121  # channels * height * width\n",
    "height, width = 157, 121  # Dimensiones espaciales\n",
    "in_channels = 25  # Combina window_size * variables climáticas\n",
    "hidden_channels = 64  # Número de canales ocultos después de la CNN\n",
    "model = ClimateModel(in_channels=in_channels, hidden_channels=hidden_channels, height=height, width=width).to(device)\n",
    "model.ode_block = ODEBlock(ODEFunc(features), features).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "train_model(model, train_loader, val_loader, epochs, optimizer, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
